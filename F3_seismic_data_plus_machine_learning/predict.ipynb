{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Function for seismic facies prediction using Convolutional Neural Nets (CNN)\n",
    "### By: Charles Rutherford Ildstad\n",
    "### Date: 22.10.2017\n",
    "### For: ConocoPhillips, Tananger,\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Make initial package imports\n",
    "import segyio\n",
    "import random\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "#\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import Conv3D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "# Set random seed for reproducability\n",
    "np.random.seed(7)\n",
    "# Confirm backend if in doubt\n",
    "#keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters for training or predicting\n",
    "filenames=['F3_entire.segy']    # name of the segy-cube(s) with data , separate by comma 'volume' for additional volumes\n",
    "inp_res = np.float32    # formatting of the input seismic (e.g. np.int8 for 8-bit data, np.float32 for 32-bit data, etc)\n",
    "cube_incr = 32    # number of increments in each direction to create a training cube\n",
    "\n",
    "# SEGY iline/xline/t dimensions:\n",
    "ils=100; ile=750; ili=1\n",
    "xls=300; xle=1250; xli=1\n",
    "ts=4; te=1848; ti=4\n",
    "    \n",
    "# Define the dictionary holding all the training parameters\n",
    "train_dict = {\n",
    "    'files' : ['multi_else_ilxl.pts','multi_grizzly_ilxl.pts','multi_high_amp_continuous_ilxl.pts',\n",
    "               'multi_high_amplitude_ilxl.pts','multi_low_amp_dips_ilxl.pts',\n",
    "               'multi_low_amplitude_ilxl.pts','multi_low_coherency_ilxl.pts',\n",
    "               'multi_salt_ilxl.pts','multi_steep_dips_ilxl.pts'],    # list of names of class-adresses\n",
    "    'num_tot_iterations': 25,    # number of times we draw a new training ensemble/mini-batch\n",
    "    'epochs' : 10,    # number of epochs we run on each training ensemble/mini-batch\n",
    "    'num_train_ex' : 20000,    # number of training examples in each training ensemble/mini-batch\n",
    "    'batch_size' : 32,    # number of training examples fed to the optimizer as a batch\n",
    "    'opt_patience' : 10,    # number of epochs with the same accuracy before force breaking the training ensemble/mini-batch\n",
    "    'data_augmentation' : False,    # whether or not we are using data augmentation\n",
    "    'save_model' : True,    # whether or not we are saving the trained model\n",
    "    'save_location' : 'F3_fullstack_multi_25i_10e_20000'    # file name for the saved trained model\n",
    "}\n",
    "\n",
    "\n",
    "# Define the dictionary holding all the prediction parameters\n",
    "pred_dict = {\n",
    "    'keras_model' :  keras.models.load_model('F3_fullstack_multi_25i_10e_20000.h5'), # input model to be used for prediction, to load a model use: keras.models.load_model('write_location')\n",
    "    #'section_edge' : np.asarray([132, 718, 400, 400, 132, 1720]), # inline and xline section to be predicted (all depths), must contain xline\n",
    "    'section_edge' : np.asarray([ils, ile, xls, xle, ts, te]),   # use the whole volume\n",
    "    'show_feature' : False,    # Show the distinct features before they are combined to a prediction\n",
    "    'xline' : 400, #123900,    # xline used for classification (index)(should be within section range)\n",
    "    'num_class' : len(train_dict['files']),    # number of classes to output\n",
    "    'cord_syst' : 'segy',    # Coordinate system used, default is 0,0. Set to 'segy' to give inputs in (inline,xline)\n",
    "    'save_pred' : True,    # Save the prediction as a segy-cube\n",
    "    'save_location' : 'F3_fullstack_multi_25i_10e_20000_facies_classes',     # file name for the saved prediction\n",
    "    'pred_batch' : 25,     # number of traces used to make batches of mini-cubes that are stored in memory at once\n",
    "    #'pred_batch' : train_dict['num_train_ex']//(pred_dict['section_edge'][5]-pred_dict['section_edge'][4])    #Suggested value\n",
    "    'pred_prob' : False     # Give the probabilities of the first class(True), or simply show where each class is classified(False)\n",
    "}\n",
    "\n",
    "# add directory prefix\n",
    "\n",
    "# 1. dir for classification data\n",
    "classification_data_dir = 'F3_classification_data'\n",
    "# actual classification data file names\n",
    "files_with_dir=[]\n",
    "for file in train_dict['files']:\n",
    "    files_with_dir.append(os.path.join(classification_data_dir, file))\n",
    "train_dict['files'] = files_with_dir\n",
    "\n",
    "# 2. dir for seismic data\n",
    "segy_data_dir = 'F3_seismic_data'\n",
    "# actual file names\n",
    "filenames_with_dir=[]\n",
    "for file in filenames:\n",
    "    filenames_with_dir.append(os.path.join(segy_data_dir, file))\n",
    "filenames = filenames_with_dir\n",
    "\n",
    "# 3. dir for output \n",
    "out_dir = 'F3_output'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "# \n",
    "train_dict['save_location'] = os.path.join(out_dir, train_dict['save_location'])\n",
    "pred_dict['save_location']  = os.path.join(out_dir, pred_dict['save_location'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ---- Functions for Input data(SEG-Y) formatting and reading ----\n",
    "# Make a function that decompresses a segy-cube and creates a numpy array, and\n",
    "# a dictionary with the specifications, like in-line range and time step length, etc.\n",
    "def segy_decomp(segy_file, plot_data = False, read_direc='xline', inp_res = np.float64):\n",
    "    # segy_file: filename of the segy-cube to be imported\n",
    "    # plot_data: boolean that determines if a random xline should be plotted to test the reading\n",
    "    # read_direc: which way the SEGY-cube should be read; 'xline', or 'inline'\n",
    "    # inp_res: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "\n",
    "    # Make an empty object to hold the output data\n",
    "    print('Starting SEG-Y decompressor')\n",
    "    output = segyio.spec()\n",
    "\n",
    "    # open the segyfile and start decomposing it\n",
    "    with segyio.open(segy_file, \"r\" ) as segyfile:\n",
    "        # Memory map file for faster reading (especially if file is big...)\n",
    "        segyfile.mmap()\n",
    "\n",
    "        # Store some initial object attributes\n",
    "        output.inl_start = segyfile.ilines[0]\n",
    "        output.inl_end = segyfile.ilines[-1]\n",
    "        output.inl_step = segyfile.ilines[1] - segyfile.ilines[0]\n",
    "\n",
    "        output.xl_start = segyfile.xlines[0]\n",
    "        output.xl_end = segyfile.xlines[-1]\n",
    "        output.xl_step = segyfile.xlines[1] - segyfile.xlines[0]\n",
    "\n",
    "        output.t_start = int(segyfile.samples[0])\n",
    "        output.t_end = int(segyfile.samples[-1])\n",
    "        output.t_step = int(segyfile.samples[1] - segyfile.samples[0])\n",
    "\n",
    "        # for qc\n",
    "        print(\" SEGY inline/xline/t geometry:\")\n",
    "        print(\" ils={}, ile={}, ili={}, ilen={}\\n\".format(output.inl_start, output.inl_end, \n",
    "                                                          output.inl_step,\n",
    "                                                          segyfile.iline.len))\n",
    "        print(\" xls={}, xle={}, xli={}, xlen={}\\n\".format(output.xl_start, output.xl_end, \n",
    "                                                          output.xl_step,\n",
    "                                                          segyfile.xline.len))\n",
    "        print(\" ts={}, te={}, ti={}\\n\".format(output.t_start, output.t_end, output.t_step))\n",
    "\n",
    "        # Pre-allocate a numpy array that holds the SEGY-cube\n",
    "        output.data = np.empty((segyfile.xline.len,segyfile.iline.len,\\\n",
    "                               (output.t_end - output.t_start)//output.t_step+1), \n",
    "                               dtype = np.float32)\n",
    "\n",
    "        # Read the entire cube line by line in the desired direction\n",
    "        if read_direc == 'inline':\n",
    "            # Potentially time this to find the \"fast\" direction\n",
    "            #start = time.time()\n",
    "            for il_index in range(segyfile.xline.len):\n",
    "                output.data[il_index,:,:] = segyfile.iline[segyfile.ilines[il_index]]\n",
    "            #end = time.time()\n",
    "            #print(end - start)\n",
    "\n",
    "        elif read_direc == 'xline':\n",
    "            # Potentially time this to find the \"fast\" direction\n",
    "            #start = time.time()\n",
    "            for xl_index in range(segyfile.iline.len):\n",
    "                output.data[:,xl_index,:] = segyfile.xline[segyfile.xlines[xl_index]]\n",
    "            #end = time.time()\n",
    "            #print(end - start)\n",
    "\n",
    "        elif read_direc == 'full':\n",
    "            ## NOTE: 'full' for some reason invokes float32 data\n",
    "            # Potentially time this to find the \"fast\" direction\n",
    "            #start = time.time()\n",
    "            output.data = segyio.tools.cube(segy_file)\n",
    "            #end = time.time()\n",
    "            #print(end - start)\n",
    "        else:\n",
    "            print('Define reading direction(read_direc) using either ''inline'', ''xline'', or ''full''')\n",
    "\n",
    "\n",
    "        # Convert the numpy array to span between -127 and 127 and convert to the desired format\n",
    "        factor = 127/np.amax(np.absolute(output.data))\n",
    "        if inp_res == np.float32:\n",
    "            output.data = (output.data*factor)\n",
    "        else:\n",
    "            output.data = (output.data*factor).astype(dtype = inp_res)\n",
    "\n",
    "        # If sepcified, plot a given x-line to test the read data\n",
    "        if plot_data:\n",
    "            # xline = 100\n",
    "            xline = np.random.randint(output.data.shape[1])\n",
    "\n",
    "            # Take a given xline\n",
    "            data = output.data[:,xline,:]\n",
    "            \n",
    "            # Plot the read x-line\n",
    "            plt.imshow(data.T,interpolation=\"nearest\", cmap=\"gray\")\n",
    "            plt.title(' xline={}'.format(xline))\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    # Return the output object\n",
    "    print('Finished using the SEG-Y decompressor')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# qc segy_decomp\n",
    "segy_obj = segy_decomp(segy_file = filenames[0],\n",
    "                       plot_data = False,\n",
    "                       read_direc = 'full',\n",
    "                       inp_res = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If sepcified, plot a given x-line to test the read data\n",
    "# Take a given xline\n",
    "xline = 300\n",
    "data = segy_obj.data[:,xline,:]\n",
    "# Plot the read x-line\n",
    "plt.imshow(data.T,interpolation=\"nearest\", cmap=\"gray\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a function that adds another layer to a segy-cube\n",
    "def segy_adder(segy_file, inp_cube, read_direc='xline', inp_res = np.float64):\n",
    "    # segy_file: filename of the segy-cube to be imported\n",
    "    # inp_cube: the existing cube that we should add a layer to\n",
    "    # cube_num: which chronological number of cube is this\n",
    "    # read_direc: which way the SEGY-cube should be read; 'xline', or 'inline'\n",
    "    # inp_res: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "\n",
    "    # Make a variable to hold the shape of the input cube and preallocate a data holder\n",
    "    print('Starting SEG-Y adder')\n",
    "    cube_shape = inp_cube.shape\n",
    "    dataholder = np.empty(cube_shape[0:-1])\n",
    "\n",
    "    # open the segyfile and start decomposing it\n",
    "    with segyio.open(segy_file, \"r\" ) as segyfile:\n",
    "        # Memory map file for faster reading (especially if file is big...)\n",
    "        segyfile.mmap()\n",
    "\n",
    "        # Read the entire cube line by line in the desired direction\n",
    "        if read_direc == 'inline':\n",
    "            # Potentially time this to find the \"fast\" direction\n",
    "            #start = time.time()\n",
    "            for il_index in range(segyfile.xline.len):\n",
    "                dataholder[il_index,:,:] = segyfile.iline[segyfile.ilines[il_index]]\n",
    "            #end = time.time()\n",
    "            #print(end - start)\n",
    "\n",
    "        elif read_direc == 'xline':\n",
    "            # Potentially time this to find the \"fast\" direction\n",
    "            #start = time.time()\n",
    "            for xl_index in range(segyfile.iline.len):\n",
    "                dataholder[:,xl_index,:] = segyfile.xline[segyfile.xlines[xl_index]]\n",
    "            #end = time.time()\n",
    "            #print(end - start)\n",
    "\n",
    "        elif read_direc == 'full':\n",
    "            ## NOTE: 'full' for some reason invokes float32 data\n",
    "            # Potentially time this to find the \"fast\" direction\n",
    "            #start = time.time()\n",
    "            dataholder[:,:,:] = segyio.tools.cube(segy_file)\n",
    "            #end = time.time()\n",
    "            #print(end - start)\n",
    "        else:\n",
    "            print('Define reading direction(read_direc) using either ''inline'', ''xline'', or ''full''')\n",
    "\n",
    "\n",
    "        # Convert the numpy array to span between -127 and 127 and convert to the desired format\n",
    "        factor = 127/np.amax(np.absolute(dataholder))\n",
    "        if inp_res == np.float32:\n",
    "            dataholder = (dataholder*factor)\n",
    "        else:\n",
    "            dataholder = (dataholder*factor).astype(dtype = inp_res)\n",
    "\n",
    "\n",
    "    # Return the output object\n",
    "    print('Finished adding a SEG-Y layer')\n",
    "    return dataholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a numpy-cube and seismic specs into a csv file/numpy-csv-format,\n",
    "def csv_struct(inp_numpy,spec_obj,section,inp_res=np.float64,save=False,savename='default_write.ixz'):\n",
    "    # inp_numpy: array that should be converted to csv\n",
    "    # spec_obj: object containing the seismic specifications, like starting depth, inlines, etc.\n",
    "    # inp_res: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "    # save: whether or not to save the output of the function\n",
    "    # savename: what to name the newly saved csv-file\n",
    "\n",
    "    # Get some initial parameters of the data\n",
    "    (ilen,xlen,zlen) = inp_numpy.shape\n",
    "    i = 0\n",
    "\n",
    "    # Preallocate the array that we want to make\n",
    "    full_np = np.empty((ilen*xlen*zlen,4),dtype = inp_res)\n",
    "\n",
    "    # Itterate through the numpy-cube and convert each trace individually to a section of csv\n",
    "    for il in range(section[0]*spec_obj.inl_step,(section[1]+1)*spec_obj.inl_step,spec_obj.inl_step):\n",
    "        j = 0\n",
    "        for xl in range(section[2]*spec_obj.xl_step,(section[3]+1)*spec_obj.xl_step,spec_obj.xl_step):\n",
    "            # Make a list of the inline number, xline number, and depth for the given trace\n",
    "            I = (il+spec_obj.inl_start)*(np.ones((zlen,1)))\n",
    "            X = (xl+spec_obj.xl_start)*(np.ones((zlen,1)))\n",
    "            Z = np.expand_dims(np.arange(section[4]*spec_obj.t_step+spec_obj.t_start,\\\n",
    "                                         (section[5]+1)*spec_obj.t_step+spec_obj.t_start,spec_obj.t_step),\\\n",
    "                               axis=1)\n",
    "\n",
    "            # Store the predicted class/probability at each og the given depths of the trace\n",
    "            D = np.expand_dims(inp_numpy[i,j,:],axis = 1)\n",
    "\n",
    "            # Concatenate these lists together and insert them into the full array\n",
    "            inp_li = np.concatenate((I,X,Z,D),axis=1)\n",
    "            full_np[i*xlen*zlen+j*zlen:i*xlen*zlen+(j+1)*zlen,:] = inp_li\n",
    "            j+=1\n",
    "        i+=1\n",
    "\n",
    "    # Add the option to save it as an external file\n",
    "    if save:\n",
    "        # save the file as the given str-name\n",
    "        np.savetxt(savename, full_np, fmt = '%f')\n",
    "\n",
    "    # Return the list of adresses and classes as a numpy array\n",
    "    return full_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ---- Functions for data augmentation ---- (Needs further development)\n",
    "# RotationXY\n",
    "def randomRotationXY(X, max_rot):\n",
    "    max_rot = 6.28318530718 / 360 * max_rot #Deg 2 rad\n",
    "    theta = tf.random_uniform([1], minval=-max_rot, maxval=max_rot, dtype='float32')\n",
    "    x = X[2] * tf.cos(theta) - X[1] * tf.sin(theta)\n",
    "    y = X[2] * tf.sin(theta) + X[1] * tf.cos(theta)\n",
    "    return tf.stack([X[0],y,x])\n",
    "\n",
    "\n",
    "# RotationZ\n",
    "def randomRotationZ(X, max_rot):\n",
    "    max_rot = 6.28318530718 / 360 * max_rot  # Deg 2 rad\n",
    "    theta = tf.random_uniform([1], minval=-max_rot, maxval=max_rot, dtype='float32')\n",
    "    t = X[0] * tf.cos(theta) - X[1] * tf.sin(theta)\n",
    "    x = X[0] * tf.sin(theta) + X[1] * tf.cos(theta)\n",
    "    return tf.stack([t,x,X[2]])\n",
    "\n",
    "\n",
    "# Stretching\n",
    "def randomStretch(window_function, strech):\n",
    "    return tf.cast(window_function,'float32') * (1 + tf.random_uniform([1],minval=-strech,maxval=strech))\n",
    "\n",
    "\n",
    "# Flip\n",
    "def randomFlip(window_function):\n",
    "    should_flip = tf.cast(tf.random_uniform([1], 0, 2, dtype=tf.int32)[0] > 0, tf.bool)\n",
    "    window_function = tf.reverse(window_function, tf.pack([should_flip]))\n",
    "    return window_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ---- Functions for the training part of the program ----\n",
    "# Make a function that combines the adress cubes and makes a list of class adresses\n",
    "def convert(file_list, save = False, savename = 'adress_list', ex_adjust = False):\n",
    "    # file_list: list of file names(strings) of adresses for the different classes\n",
    "    # save: boolean that determines if a new ixz file should be saved with adresses and class numbers\n",
    "    # savename: desired name of new .ixz-file\n",
    "    # ex_adjust: boolean that determines if the amount of each class should be approximately equalized\n",
    "\n",
    "    # Make an array of that holds the number of each example provided, if equalization is needed\n",
    "    if ex_adjust:\n",
    "        len_array = np.zeros(len(file_list),dtype = np.float32)\n",
    "        for i in range(len(file_list)):\n",
    "            len_array[i] = len(np.loadtxt(file_list[i], skiprows=0, usecols = range(3), dtype = np.float32))\n",
    "\n",
    "        # Cnvert this array to a multiplier that determines how many times a given class set needs to be\n",
    "        len_array /= max(len_array)\n",
    "        multiplier = 1//len_array\n",
    "\n",
    "\n",
    "    # preallocate space for the adr_list, the output containing all the adresses and classes\n",
    "    adr_list = np.empty([0,4], dtype = np.int32)\n",
    "\n",
    "    # Itterate through the list of example adresses and store the class as an integer\n",
    "    for i in range(len(file_list)):\n",
    "        a = np.loadtxt(file_list[i], skiprows=0, usecols = range(3), dtype = np.int32)\n",
    "        adr_list = np.append(adr_list,np.append(a,i*np.ones((len(a),1),dtype = np.int32),axis=1),axis=0)\n",
    "\n",
    "        # If desired copy the entire list by the multiplier calculated\n",
    "        if ex_adjust:\n",
    "            for k in range(int(multiplier[i])-1):\n",
    "                adr_list = np.append(adr_list,np.append(a,i*np.ones((len(a),1),dtype = np.int32),axis=1),axis=0)\n",
    "\n",
    "    # Add the option to save it as an external file\n",
    "    if save:\n",
    "        # save the file as the given str-name\n",
    "        np.savetxt(savename + '.ixz', adr_list, fmt = '%i')\n",
    "\n",
    "    # Return the list of adresses and classes as a numpy array\n",
    "    return adr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert ASCII mask files to volume\n",
    "def class_to_volume_mask(seis_spec, label_list):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "      seis_spec:  object that holds the specifications of the seismic cube\n",
    "      label_list: list of file names(strings) of adresses for the different classes\n",
    "      \n",
    "    Returns:\n",
    "      mask volume with labels\n",
    "    \"\"\"\n",
    "    # Make the list of class data\n",
    "    print('Making class-adresses')\n",
    "\n",
    "    # class_array: 4-column numpy matrix that adress and class information\n",
    "    class_array = convert(file_list = label_list,\n",
    "                          save = False,\n",
    "                          savename = None,\n",
    "                          ex_adjust = True)\n",
    "\n",
    "    # Define some boundary parameters given in the input object\n",
    "    inline_start = seis_spec.inl_start\n",
    "    inline_end = seis_spec.inl_end\n",
    "    inline_step = seis_spec.inl_step\n",
    "    #\n",
    "    xline_start = seis_spec.xl_start\n",
    "    xline_end = seis_spec.xl_end\n",
    "    xline_step = seis_spec.xl_step\n",
    "    #\n",
    "    t_start = seis_spec.t_start\n",
    "    t_end = seis_spec.t_end\n",
    "    t_step = seis_spec.t_step\n",
    "\n",
    "    # volume shape\n",
    "    ny = (inline_end-inline_start)//inline_step + 1\n",
    "    nx = (xline_end-xline_start)//xline_step + 1\n",
    "    nt = (t_end-t_start)//t_step + 1\n",
    "    \n",
    "    # mask_arr: 3D numpy array that holds a seismic mask\n",
    "    mask_arr = np.zeros((ny, nx, nt), dtype=np.int8)\n",
    "    \n",
    "        # Make the list of class data\n",
    "    print('Converting class-adresses to mask volume')\n",
    "\n",
    "    # build mask volume\n",
    "    for row in class_array:\n",
    "        y, x, t, label = row\n",
    "        #\n",
    "        iy = (y - inline_start)//inline_step\n",
    "        ix = (x - xline_start)//xline_step\n",
    "        it = (t - t_start)//t_step\n",
    "        #\n",
    "        mask_arr[iy, ix, it] = label\n",
    "        \n",
    "    return mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SEG-Y decompressor\n",
      " SEGY inline/xline/t geometry:\n",
      " ils=100, ile=750, ili=1, ilen=951\n",
      "\n",
      " xls=300, xle=1250, xli=1, xlen=651\n",
      "\n",
      " ts=4, te=1848, ti=4\n",
      "\n",
      "Finished using the SEG-Y decompressor\n"
     ]
    }
   ],
   "source": [
    "# load seismic volume\n",
    "label_list = train_dict['files']\n",
    "segy_obj = segy_decomp(segy_file = filenames[0], plot_data = False, read_direc = 'full', inp_res = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making class-adresses\n",
      "Converting class-adresses to mask volume\n",
      "(651, 951, 462)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 0, 121041)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label to volume\n",
    "mask = class_to_volume_mask(segy_obj, label_list)\n",
    "\n",
    "print(mask.shape)\n",
    "np.max(mask[239, :, :]), np.min(mask[239, :, :]), np.count_nonzero(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "def generate_color_map(N=256, normalized=False):\n",
    "    \"\"\"from https://gist.github.com/wllhf/a4533e0adebe57e3ed06d4b50c8419ae .\"\"\"\n",
    "    def bitget(byteval, idx):\n",
    "        return ((byteval & (1 << idx)) != 0)\n",
    "\n",
    "    dtype = 'float32' if normalized else 'uint8'\n",
    "    cmap = np.zeros((N, 3), dtype=dtype)\n",
    "    for i in range(N):\n",
    "        r = g = b = 0\n",
    "        c = i\n",
    "        for j in range(8):\n",
    "            r = r | (bitget(c, 0) << 7 - j)\n",
    "            g = g | (bitget(c, 1) << 7 - j)\n",
    "            b = b | (bitget(c, 2) << 7 - j)\n",
    "            c = c >> 3\n",
    "\n",
    "        cmap[i] = np.array([r, g, b])\n",
    "\n",
    "    cmap = cmap / 255 if normalized else cmap\n",
    "    return cmap\n",
    "#\n",
    "cmap = generate_color_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(462, 951, 3)\n"
     ]
    }
   ],
   "source": [
    "mask_il_239=mask[239, :, :]\n",
    "segy_data_239=segy_obj.data[239, :, :]\n",
    "\n",
    "#\n",
    "mask1 = np.zeros((mask_il_239.shape[1], mask_il_239.shape[0], 3))\n",
    "for i in range(mask_il_239.shape[0]):\n",
    "    for j in range(mask_il_239.shape[1]):\n",
    "        mask1[j, i, :] = cmap[mask_il_239[i][j]]\n",
    "print(mask1.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f615008d160>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADFCAYAAABaSzmEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYrFYSpz/D/FIIl18Hf5YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f61500bf080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.imshow((mask_il_239.T+segy_data_239.T)*0.5, vmin=0, vmax=8, interpolation='bicubic')\n",
    "plt.imshow(mask1, interpolation='bicubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for example creating\n",
    "# Outputs a dictionary with pairs of cube tuples and labels\n",
    "def ex_create(adr_arr,seis_arr,seis_spec,num_examp,cube_incr,inp_res=np.float64,sort_adr = False,replace_illegals = True):\n",
    "    # adr_arr: 4-column numpy matrix that holds a header in the first row, then adress and class information for examples\n",
    "    # seis_arr: 3D numpy array that holds a seismic cube\n",
    "    # seis_spec: object that holds the specifications of the seismic cube;\n",
    "    # num_examp: the number of output mini-cubes that should be created\n",
    "    # cube_incr: the number of increments included in each direction from the example to make a mini-cube\n",
    "    # inp_res: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "    # sort_adr: boolean; whether or not to sort the randomly drawn adresses before making the example cubes\n",
    "    # replace_illegals: boolean; whether or not to draw a new sample in place for an illegal one, or not\n",
    "\n",
    "    # Define the cube size\n",
    "    cube_size = 2*cube_incr+1\n",
    "\n",
    "    # Define some boundary parameters given in the input object\n",
    "    inline_start = seis_spec.inl_start\n",
    "    inline_end = seis_spec.inl_end\n",
    "    inline_step = seis_spec.inl_step\n",
    "    #\n",
    "    xline_start = seis_spec.xl_start\n",
    "    xline_end = seis_spec.xl_end\n",
    "    xline_step = seis_spec.xl_step\n",
    "    #\n",
    "    t_start = seis_spec.t_start\n",
    "    t_end = seis_spec.t_end\n",
    "    t_step = seis_spec.t_step\n",
    "    #\n",
    "    num_channels = seis_spec.cube_num\n",
    "\n",
    "    # Define the buffer zone around the edge of the cube that defines the legal/illegal adresses\n",
    "    inl_min = inline_start + inline_step*cube_incr\n",
    "    inl_max = inline_end - inline_step*cube_incr\n",
    "    #\n",
    "    xl_min = xline_start + xline_step*cube_incr\n",
    "    xl_max = xline_end - xline_step*cube_incr\n",
    "    #\n",
    "    t_min = t_start + t_step*cube_incr\n",
    "    t_max = t_end - t_step*cube_incr\n",
    "\n",
    "    # Print the buffer zone edges\n",
    "    print('Defining the buffer zone:')\n",
    "    print('(inl_min,','inl_max,','xl_min,','xl_max,','t_min,','t_max)')\n",
    "    print('(',inl_min,',',inl_max,',',xl_min,',',xl_max,',',t_min,',',t_max,')')\n",
    "    \n",
    "    # Also give the buffer values in terms of indexes\n",
    "    print('(',cube_incr,',',((inline_end-inline_start)//inline_step) - cube_incr,\\\n",
    "          ',',cube_incr,',',((xline_end-xline_start)//xline_step) - cube_incr,\\\n",
    "          ',',cube_incr,',',((t_end-t_start)//t_step) - cube_incr,')')\n",
    "\n",
    "    # We preallocate the function outputs; a list of examples and a list of labels\n",
    "    examples = np.empty((num_examp,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "    labels = np.empty(num_examp,dtype=np.int8)\n",
    "\n",
    "    # If we want to stack the examples in the third dimension we use the following example preallocation in stead\n",
    "    # examples = np.empty((num_examp*(cube_size),(cube_size),(cube_size)),dtype=inp_res)\n",
    "\n",
    "    # Generate a random list of indexes to be drawn, and make sure it only takes a legal amount of examples\n",
    "    try:\n",
    "        max_row_idx = len(adr_arr)-1\n",
    "        rand_idx = random.sample(range(0, max_row_idx), num_examp)\n",
    "        # NOTE: Could be faster to sort indexes before making examples for algorithm optimization\n",
    "        if sort_adr:\n",
    "            rand_idx.sort()\n",
    "    except ValueError:\n",
    "        print('Sample size exceeded population size.')\n",
    "\n",
    "    # Make an iterator for when the lists should become shorter(if we have replacement of illegals or not)\n",
    "    n=0\n",
    "    for i in range(num_examp):\n",
    "        # Get a random in-line, x-line, and time value, and store the label\n",
    "        # Make sure there is room for an example at this index\n",
    "        for j in range(50):\n",
    "            adr = adr_arr[rand_idx[i]]\n",
    "            # Check that the given example is within the legal zone\n",
    "            if (adr[0]>=inl_min and adr[0]<inl_max) and \\\n",
    "                (adr[1]>=xl_min and adr[1]<xl_max) and \\\n",
    "                (adr[2]>=t_min and adr[2]<t_max):\n",
    "                # Make the example for the given address\n",
    "                # Convert the adresses to indexes and store the examples in the 4th dimension\n",
    "                idx = [(adr[0]-inline_start)//inline_step,(adr[1]-xline_start)//xline_step,(adr[2]-t_start)//t_step]\n",
    "\n",
    "\n",
    "                examples[i-n,:,:,:,:] = seis_arr[idx[0]-cube_incr:idx[0]+cube_incr+1,\\\n",
    "                                                 idx[1]-cube_incr:idx[1]+cube_incr+1,\\\n",
    "                                                 idx[2]-cube_incr:idx[2]+cube_incr+1,:]\n",
    "\n",
    "                # Put the cube and label into the lists\n",
    "                labels[i-n] = adr[-1]\n",
    "\n",
    "                # Alternatively; stack the examples in the third dimension\n",
    "                #datasets[(i-n)*(cube_size):(i-n+1)*(cube_size),:,:] = ex\n",
    "                break\n",
    "            else:\n",
    "                # If we want to replace the illegals, draw again\n",
    "                if replace_illegals:\n",
    "                    rand_idx[i] = random.randint(0,max_row_idx)\n",
    "                else:\n",
    "                    # if not, just make the output lists shorter\n",
    "                    n += 1\n",
    "                    break\n",
    "\n",
    "            if j == 50:\n",
    "                # If we can't get a proper cube in 50 consequtive tries\n",
    "                print('Badly conditioned dataset!')\n",
    "\n",
    "    # Slice the data if desired\n",
    "    #labels = labels[0:i-n+1]\n",
    "    #examples = examples[0:i-n+1,:,:,:]\n",
    "\n",
    "    # Return the output list/tuple (slice it if it has been shortened)\n",
    "    return (examples[0:i-n+1,:,:,:,:], labels[0:i-n+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that takes the epoch as input and returns the desired learning rate\n",
    "def adaptive_lr(input_int):\n",
    "    # input_int: the epoch that is currently being entered\n",
    "\n",
    "    # define the learning rate (quite arbitrarily decaying)\n",
    "    lr = 0.1**input_int\n",
    "\n",
    "    #return the learning rate\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Make the network structure and outline, and train it\n",
    "def train_model(segy_obj,class_array,num_classes,cube_incr,inp_res = np.float64,\\\n",
    "                num_bunch = 10,num_epochs = 100,num_examples = 10000,batch_size = 32,\\\n",
    "                opt_patience = 5, data_augmentation=False,num_channels = 1,\\\n",
    "                keras_model = None,write_out = False,write_location = 'default_write'):\n",
    "    # segy_obj: Object returned from the segy_decomp function\n",
    "    # class_array: numpy array of class adresses and type, returned from the convert function\n",
    "    # num_classes: number of destinct classes we are training on\n",
    "    # cube_incr: number of increments included in each direction from the example to make a mini-cube\n",
    "    # inp_res: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "    # num_bunch: number of times we draw a new ensemble of training data and train on it\n",
    "    # num_epochs: number of epochs we train on a given ensemble of training data\n",
    "    # num_examples: number of examples we draw in an ensemble\n",
    "    # batch_size: number of mini-batches we go through at a time from the number of examples\n",
    "    # opt_patience: epochs that can pass without improvement in accuracy before the system breaks the loop\n",
    "    # data_augmentation: boolean which determines whether or not to apply augmentation on the examples\n",
    "    # num_channels: number of segy-cubes we have imported simultaneously\n",
    "    # keras_model: existing keras model to be improved if the user wants to improve and not create a new model\n",
    "    # write_out: boolean; save the trained model to disk or not,\n",
    "    # write_location: desired location on the disk for the model to be saved\n",
    "\n",
    "    # Check if the user wants to make a new model, or train an existing input model\n",
    "    if keras_model == None:\n",
    "        # Begin setting up model architecture and parameters\n",
    "        cube_size = 2*cube_incr+1\n",
    "\n",
    "        #  This model is loosely built after that of Anders Waldeland (5 Convolutional layers\n",
    "        #  and 2 fully connected layers with rectified linear and softmax activations)\n",
    "        # We have added drop out and batch normalization our selves, and experimented with multi-prediction\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(50, (5, 5, 5), padding='same', input_shape=(cube_size,cube_size,cube_size,num_channels), \\\n",
    "                         strides=(4, 4, 4), data_format=\"channels_last\",name = 'conv_layer1'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv3D(50, (3, 3, 3), strides=(2, 2, 2), padding = 'same',name = 'conv_layer2'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv3D(50, (3, 3, 3), strides=(2, 2, 2), padding= 'same',name = 'conv_layer3'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv3D(50, (3, 3, 3), strides=(2, 2, 2), padding= 'same',name = 'conv_layer4'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv3D(50, (3, 3, 3), strides=(2, 2, 2), padding= 'same',name = 'conv_layer5'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50,name = 'dense_layer1'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(10,name = 'attribute_layer'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(num_classes, name = 'pre-softmax_layer'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        # initiate the Adam optimizer with a given learning rate (Note that this is adapted later)\n",
    "        opt = keras.optimizers.adam(lr=0.001)\n",
    "\n",
    "        # Compile the model with the desired loss, optimizer, and metric\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    else:\n",
    "        # Define the model we are performing training on as the input model\n",
    "        model = keras_model\n",
    "\n",
    "    ### Begin actual model training\n",
    "    # Define some initial parameters, and the early stopping and adaptive learning rate callback\n",
    "    early_stopping = EarlyStopping(monitor='acc', patience=opt_patience)\n",
    "    LR_sched = LearningRateScheduler(schedule = adaptive_lr)\n",
    "\n",
    "    # Potential for adding tensor board functionality to see the change of parameters with time\n",
    "    #tensor_board = TensorBoard(log_dir='./logs', histogram_freq=1, batch_size=32,\\\n",
    "    #                            write_graph=True, write_grads=True, write_images=True,\\\n",
    "    #                            embeddings_freq=1, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "    # Start the timer for the training iterations\n",
    "    start = time.time()\n",
    "\n",
    "    # Train the model\n",
    "    for i in range(num_bunch):\n",
    "        # Give an update as to how many times we have drawn a new example set\n",
    "        print('Iteration number:',i+1,'/',num_bunch)\n",
    "\n",
    "        # Make the examples\n",
    "        print('Starting training data creation:')\n",
    "        (x_train, y_train) = ex_create(adr_arr = class_array,\n",
    "                                       seis_arr = segy_obj.data,\n",
    "                                       seis_spec = segy_obj,\n",
    "                                       num_examp = num_examples,\n",
    "                                       cube_incr = cube_incr,\n",
    "                                       inp_res = inp_res,\n",
    "                                       sort_adr = False,\n",
    "                                       replace_illegals = True)\n",
    "\n",
    "        print('Finished creating',num_examples,'examples!')\n",
    "\n",
    "        # Define and reshape the training data\n",
    "        # x_train = np.expand_dims(x_train,axis=4)\n",
    "\n",
    "        # Convert labels to one-hot encoding(and if necessary change the data type and scale as needed)\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "        # See if the user has chosen to implement data_augmentation and implement it if so\n",
    "        if not data_augmentation:\n",
    "            print('Not using data augmentation.')\n",
    "            # Run the model training\n",
    "            history = model.fit(x=x_train,\n",
    "                                y=y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                validation_split=0.2,\n",
    "                                callbacks=[early_stopping, LR_sched],\n",
    "                                epochs=num_epochs,\n",
    "                                shuffle=True)\n",
    "\n",
    "        else:\n",
    "            # !!! Currently does not work\n",
    "            print('Using real-time data augmentation.')\n",
    "            # This will do preprocessing and realtime data augmentation\n",
    "            datagen = ImageDataGenerator(featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                                         samplewise_center=False,  # set each sample mean to 0\n",
    "                                         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                                         samplewise_std_normalization=False,  # divide each input by its std\n",
    "                                         zca_whitening=False,  # apply ZCA whitening\n",
    "                                         rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                                         width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "                                         height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "                                         horizontal_flip=True,  # randomly flip images\n",
    "                                         vertical_flip=False,    # randomly flip images\n",
    "                                         shear_range = 0.349, # shear intensity (counter-clockwise direction in radians)\n",
    "                                         zoom_range = 0.2,   # range for random zoom (float)\n",
    "                                         rescale = 1.5)  # rescaling factor which multiplies data by the value provided\n",
    "\n",
    "            # Compute quantities required for feature-wise normalization\n",
    "            # (std, mean, and principal components if ZCA whitening is applied).\n",
    "            datagen.fit(x_train)\n",
    "\n",
    "            # Fit the model on the batches generated by datagen.flow().\n",
    "            history = model.fit_generator(datagen.flow(x_train,\n",
    "                                                       y_train,\n",
    "                                                       batch_size = batch_size),\n",
    "                                          steps_per_epoch = x_train.shape[0] // batch_size,\n",
    "                                          epochs = num_epochs,\n",
    "                                          validation_data = (x_test, y_test))\n",
    "\n",
    "        # Print the training summary\n",
    "        print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "        # Set the time for one training iteration\n",
    "        if i == 0:\n",
    "            end = time.time()\n",
    "            tot_time = (end-start)*num_bunch\n",
    "\n",
    "\n",
    "\n",
    "        # Give an update on the time remaining\n",
    "        rem_time = ((num_bunch-(i+1))/num_bunch)*tot_time\n",
    "\n",
    "        if rem_time <= 300:\n",
    "            print('Approximate time remaining of the training:',rem_time,' sec.')\n",
    "        elif 300 < rem_time <= 60*60:\n",
    "            minutes = rem_time//60\n",
    "            seconds = (rem_time%60)*(60/100)\n",
    "            print('Approximate time remaining of the training:',minutes,' min., ',seconds,' sec.')\n",
    "        elif 60*60 < rem_time <= 60*60*24:\n",
    "            hours = rem_time//(60*60)\n",
    "            minutes = (rem_time%(60*60))*(1/60)*(60/100)\n",
    "            print('Approximate time remaining of the training:',hours,' hrs., ',minutes,' min., ')\n",
    "        else:\n",
    "            days = time_rem//(24*60*60)\n",
    "            hours = (time_rem%(24*60*60))*(1/60)*((1/60))*(24/100)\n",
    "            print('Approximate time remaining of the training:',days,' days, ',hours,' hrs., ')\n",
    "\n",
    "\n",
    "    # Save the trained model if the user has chosen to do so\n",
    "    if write_out:\n",
    "        print('Saving model: ...')\n",
    "        model.save(write_location + '.h5')\n",
    "        print('Model saved.')\n",
    "\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ---- Functions for the prediction part of the program ----\n",
    "# Parse the cube into sub-cubes suitable as model input\n",
    "def cube_parse(seis_arr,cube_incr,inp_res = np.float64, mode = 'trace', padding = False,\\\n",
    "               conc = False, inline_num = 0, xline_num = 0, depth = 0):\n",
    "    # seis_arr: a 3D numpy array that holds a seismic cube\n",
    "    # cube_incr: number of increments included in each direction from the example to make a mini-cube\n",
    "    # inp_res: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "    # mode: how much of the 3D-cube should be converted to examples ('full','xline','inline','trace', or 'point')\n",
    "    # padding: do we want to pad the zone which is outside our buffer with zeroes?\n",
    "    # conc: do we want to concattenate the examples, or store them in the same matrix they were fed to us?\n",
    "    # inline_num: if mode is inline or point; what inline do we use?\n",
    "    # xline_num: if mode is xline or point; what xline do we use?\n",
    "    # depth: if mode is point; what depth do we use?\n",
    "\n",
    "    # Make some initial definitions wrt. dimensionality\n",
    "    inls = seis_arr.shape[0]\n",
    "    xls = seis_arr.shape[1]\n",
    "    zls = seis_arr.shape[2]\n",
    "    num_channels = seis_arr.shape[3]\n",
    "    #\n",
    "    cube_size = 2*cube_incr+1\n",
    "\n",
    "    # Define the indent where the saved data will start, if user wants padding this is 0, else it is cube_incr\n",
    "    if padding:\n",
    "        i_re = 0\n",
    "        x_re = 0\n",
    "        z_re = 0\n",
    "        # Preallocate the output array, if concatenated it's 4 dimensional, if not it's 6 dimensional\n",
    "        if conc:\n",
    "            # Make adjustments to the parameters so that we iterate over the right number of samples, etc.\n",
    "            if mode == 'full':\n",
    "                examples = np.zeros((inls*xls*zls,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "            elif mode == 'inline':\n",
    "                examples = np.zeros((xls*zls,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                x_re = cube_incr\n",
    "            elif mode == 'xline':\n",
    "                examples = np.zeros((inls*zls,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                i_re = cube_incr\n",
    "            elif mode == 'trace':\n",
    "                examples = np.zeros((zls,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                i_re = cube_incr\n",
    "                x_re = cube_incr\n",
    "            elif mode == 'point':\n",
    "                examples = np.zeros((1,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                i_re = cube_incr\n",
    "                x_re = cube_incr\n",
    "                z_re = cube_incr\n",
    "            else:\n",
    "                print('ERROR: invalid mode! use: ''full'',''xline'',''inline'',''trace'', or ''point''')\n",
    "            # Take into account that we will have a total smaller dimensionality of data due to illegals\n",
    "            inls -= 2*cube_incr\n",
    "            xls -= 2*cube_incr\n",
    "            zls -= 2*cube_incr\n",
    "        else:\n",
    "            # Make adjustments to the parameters so that we iterate over the right number of samples, etc.\n",
    "            if mode == 'full':\n",
    "                examples = np.zeros((inls,xls,zls,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "            elif mode == 'inline':\n",
    "                examples = np.zeros((1,xls,zls,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                x_re = cube_incr\n",
    "            elif mode == 'xline':\n",
    "                examples = np.zeros((inls,1,zls,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                i_re = cube_incr\n",
    "            elif mode == 'trace':\n",
    "                examples = np.zeros((1,1,zls,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                i_re = cube_incr\n",
    "                x_re = cube_incr\n",
    "            elif mode == 'point':\n",
    "                examples = np.zeros((1,1,1,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                i_re = cube_incr\n",
    "                x_re = cube_incr\n",
    "                z_re = cube_incr\n",
    "            else:\n",
    "                print('ERROR: invalid mode! use: ''full'',''xline'',''inline'',''trace'', or ''point''')\n",
    "    else:\n",
    "        i_re = cube_incr\n",
    "        x_re = cube_incr\n",
    "        z_re = cube_incr\n",
    "        # Preallocate the output array, if concatenated it's 5 dimensional, if not it's 7 dimensional\n",
    "        if conc:\n",
    "            # Make adjustments to the parameters so that we iterate over the right number of samples, etc.\n",
    "            if mode == 'full':\n",
    "                examples = np.empty(((inls-2*cube_incr)*(xls-2*cube_incr)*(zls-2*cube_incr),cube_size,cube_size,cube_size,num_channels),\\\n",
    "                                   dtype=inp_res)\n",
    "            elif mode == 'inline':\n",
    "                examples = np.empty(((xls-2*cube_incr)*(zls-2*cube_incr),cube_size,cube_size,cube_size,num_channels),\\\n",
    "                                   dtype=inp_res)\n",
    "                inline_num -= cube_incr\n",
    "                xline_num = 0\n",
    "                depth = 0\n",
    "            elif mode == 'xline':\n",
    "                examples = np.empty(((inls-2*cube_incr)*(zls-2*cube_incr),cube_size1,cube_size,cube_size,num_channels),\\\n",
    "                                   dtype=inp_res)\n",
    "                inline_num = 0\n",
    "                xline_num -= cube_incr\n",
    "                depth = 0\n",
    "            elif mode == 'trace':\n",
    "                examples = np.empty((zls-2*cube_incr,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                inline_num -= cube_incr\n",
    "                xline_num -= cube_incr\n",
    "                depth = 0\n",
    "            elif mode == 'point':\n",
    "                examples = np.empty((1,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "                inline_num -= cube_incr\n",
    "                xline_num -= cube_incr\n",
    "                depth -= cube_incr\n",
    "            else:\n",
    "                print('ERROR: invalid mode! use: ''full'',''xline'',''inline'',''trace'', or ''point''')\n",
    "            # Take into account that we will have a total smaller dimensionality of data due to illegals\n",
    "            inls -= 2*cube_incr\n",
    "            xls -= 2*cube_incr\n",
    "            zls -= 2*cube_incr\n",
    "        else:\n",
    "            if mode == 'full':\n",
    "                examples = np.empty(((inls-2*cube_incr),(xls-2*cube_incr),(zls-2*cube_incr),cube_size,cube_size,cube_size,num_channels),\\\n",
    "                                   dtype=inp_res)\n",
    "            elif mode == 'inline':\n",
    "                examples = np.empty((1,(xls-2*cube_incr),(zls-2*cube_incr),cube_size,cube_size,cube_size,num_channels),\\\n",
    "                                   dtype=inp_res)\n",
    "            elif mode == 'xline':\n",
    "                examples = np.empty(((inls-2*cube_incr),1,(zls-2*cube_incr),cube_size,cube_size,cube_size,num_channels),\\\n",
    "                                   dtype=inp_res)\n",
    "            elif mode == 'trace':\n",
    "                examples = np.empty((1,1,(zls-2*cube_incr),cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "            elif mode == 'point':\n",
    "                examples = np.empty((1,1,1,cube_size,cube_size,cube_size,num_channels),dtype=inp_res)\n",
    "            else:\n",
    "                print('ERROR: invalid mode! use: ''full'',''xline'',''inline'',''trace'', or ''point''')\n",
    "\n",
    "\n",
    "    # Iterate through the desired section of the 3D input array, create the example cubes, and store them as desired\n",
    "    if conc:\n",
    "        # Make the cubes\n",
    "        for i in range(cube_incr, inls+cube_incr):\n",
    "            if mode == 'xline':\n",
    "                j = xline_num\n",
    "                for k in range(cube_size, zls+cube_size):\n",
    "                    examples[inls*(i-i_re)+k-z_re,:,:,:,:] = seis_arr[i-cube_incr+inline_num:i+cube_incr+inline_num+1,\\\n",
    "                                                                      j-cube_incr:j+cube_incr+1,\\\n",
    "                                                                      k-cube_incr+depth:k+cube_incr+depth+1,:]\n",
    "            else:\n",
    "                for j in range(cube_incr, xls+cube_incr):\n",
    "                    for k in range(cube_incr, zls+cube_incr):\n",
    "                        #print('---------cube_incr={}, inline_num={}\\n'.format(cube_incr, inline_num))\n",
    "                        cc = seis_arr[i-cube_incr+inline_num:i+cube_incr+inline_num+1,\\\n",
    "                                      j-cube_incr+xline_num:j+cube_incr+xline_num+1,\\\n",
    "                                      k-cube_incr+depth:k+cube_incr+depth+1,:]\n",
    "                        #print(' examples shape:{}, cc shape:{} \\n'.format(examples.shape, cc.shape))\n",
    "                        #\n",
    "                        examples[(i-i_re)*inls+(j-x_re)*xls+k-z_re,:,:,:,:] = cc\n",
    "\n",
    "                        # Make sure we stop after the appropriate number of iterations\n",
    "                        if mode == 'point':\n",
    "                            break\n",
    "                    if mode == 'point' or mode == 'trace':\n",
    "                        break\n",
    "                if mode == 'point' or mode == 'trace' or mode == 'inline':\n",
    "                    break\n",
    "\n",
    "\n",
    "    else:\n",
    "        # Make the cubes\n",
    "        for i in range(cube_incr, inls-cube_incr):\n",
    "            if mode == 'xline':\n",
    "                for k in range(cube_incr, zls-cube_incr):\n",
    "                    examples[i-i_re,1,k-z_re,:,:,:,:] = seis_arr[i-cube_incr:i+cube_incr+1,\\\n",
    "                                                                 xline_num-cube_incr:xline_num+cube_incr+1,\\\n",
    "                                                                 k-cube_incr:k+cube_incr+1,:]\n",
    "            else:\n",
    "                for j in range(cube_incr, xls-cube_incr):\n",
    "                    for k in range(cube_incr, zls-cube_incr):\n",
    "                        examples[i-i_re,j-x_re,k-z_re,:,:,:,:] = seis_arr[i+inline_num-cube_incr:i+inline_num+cube_incr+1,\\\n",
    "                                                                          j+xline_num-cube_incr:j+xline_num+cube_incr+1,\\\n",
    "                                                                          k+depth-cube_incr:k+depth+cube_incr+1,:]\n",
    "\n",
    "                        # Make sure we stop after the appropriate number of iterations\n",
    "                        if mode == 'point':\n",
    "                            break\n",
    "                    if mode == 'point' or mode == 'trace':\n",
    "                        break\n",
    "                if mode == 'point' or mode == 'trace' or mode == 'inline':\n",
    "                    break\n",
    "\n",
    "\n",
    "    # Return the list of examples stored as the desired type of array\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n",
    "# Make an intermediate output model to check filters\n",
    "def makeIntermediate(keras_model,layer_name):\n",
    "    # keras_model: keras model that has been trained previously\n",
    "    # layer_name: name of the layer with the desired output\n",
    "\n",
    "    # Define the new model that stops at the desired layer\n",
    "    intermediate_layer_model = Model(inputs=keras_model.input,\\\n",
    "                                     outputs=keras_model.get_layer(layer_name).output)\n",
    "\n",
    "    # Return the newly defined model\n",
    "    return intermediate_layer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the output class of the given input traces\n",
    "def predicting(filename,inp_seis,seis_obj,keras_model,cube_incr,num_classes,inp_res=np.float64,mode='xline',\\\n",
    "               section=np.asarray([0,0,0,0,0,0]),line_num=0, print_segy = False,savename = 'default_write',\\\n",
    "               pred_batch = 1,show_features = False, layer_name='attribute_layer', show_prob = False):\n",
    "    # filename: filename of the segy-cube to be imported (necessary for copying the segy-frame before writing a new segy)\n",
    "    # inp_seis: a 3D numpy array that holds the input seismic cube\n",
    "    # seis_obj: Object returned from the segy_decomp function\n",
    "    # keras_model: keras model that has been trained previously\n",
    "    # cube_incr: number of increments included in each direction from the example to make a mini-cube\n",
    "    # num_classes: num_classes: number of destinct classes we are training on\n",
    "    # inp_res: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "    # mode: what part of the cube to predict along; 'inline', 'xline', 'section, or 'full' (entire cube)\n",
    "    # section: edge locations(index) of the sub-section (min. inline, max. inline, min. xline, max xline, min z, max z)\n",
    "    # line_num: xline/inline number to predict along\n",
    "    # print_segy: whether or not to save the prediction as a segy, npy and csv file (previously just segy)\n",
    "    # savename: name of the files to be saved (extensions are added automatically)\n",
    "    # pred_batch: number of traces to predict on at a time\n",
    "    # show_features: whether or not to get the features or the classes\n",
    "    # layer_name: optionally give a different layer to get the features from (name defined in keras.model)\n",
    "    # show_prob: if the user wants to get out probabilities or classifications\n",
    "\n",
    "    # Define some initial parameters\n",
    "    num_channels = seis_obj.cube_num\n",
    "    inls = inp_seis.shape[0]\n",
    "    xls = inp_seis.shape[1]\n",
    "    zls = inp_seis.shape[2]\n",
    "    #\n",
    "    cube_size = 2*cube_incr+1\n",
    "\n",
    "    # If the user simply wants the classification we only need 1 value for each input point,\n",
    "    if not show_prob:\n",
    "        num_classes = 1\n",
    "\n",
    "    # Read the section needed for prediction depending on the mode\n",
    "    if mode == 'inline':\n",
    "        section_edge = np.asarray([line_num,line_num,cube_incr,xls-cube_incr,cube_incr,zls-cube_incr])\n",
    "    elif mode == 'xline':\n",
    "        section_edge = np.asarray([cube_incr,inls-cube_incr,line_num,line_num,cube_incr,zls-cube_incr])\n",
    "    elif mode == 'section':\n",
    "        section_edge = section\n",
    "    elif mode == 'full':\n",
    "        section_edge = np.asarray([cube_incr,inls-cube_incr,cube_incr,xls-cube_incr,cube_incr,zls-cube_incr])\n",
    "    else:\n",
    "        print('invalid mode, please input inline, xline, section, or full')\n",
    "\n",
    "    # Preallocate the full prediction array and if the user wants to show the features make the intermediate model,\n",
    "    if show_features:\n",
    "        intermediate_layer_model = Model(inputs=keras_model.input,\n",
    "                                         outputs=keras_model.get_layer(layer_name).output)\n",
    "        prediction = np.empty((\\\n",
    "            (section_edge[5]-section_edge[4]+1)*(section_edge[3]-section_edge[2]+1)*(section_edge[1]-section_edge[0]+1),10),\\\n",
    "                              dtype=np.float32)\n",
    "    else:\n",
    "        prediction = np.empty((\\\n",
    "            (section_edge[5]-section_edge[4]+1)*(section_edge[3]-section_edge[2]+1)*(section_edge[1]-section_edge[0]+1),\\\n",
    "                               num_classes),dtype=np.float32)\n",
    "\n",
    "    # Preallocate the data array to fill for each batch and initiate iterators\n",
    "    data = np.empty((pred_batch*(section_edge[5]-section_edge[4]+1),cube_size,cube_size,cube_size,num_channels), dtype=inp_res)\n",
    "    indx = 0\n",
    "    jndx = 0\n",
    "\n",
    "    # Calculate how many sets of batches need to be done and define parameters needed for the final batch\n",
    "    tot_len = (section[1]-section[0]+1)*(section[3]-section[2]+1)\n",
    "    rem = tot_len % pred_batch\n",
    "    num_it = tot_len // pred_batch\n",
    "    # Time the sub_prediction\n",
    "    start = time.time()\n",
    "\n",
    "    # Start making sub-cubes from the input traces and store then in the data array\n",
    "    print('Retrieving to memory:')\n",
    "    for il_num in range(section_edge[0],section_edge[1]+1):\n",
    "        # Make a progres update for the inline number\n",
    "        print('inline-num:',il_num-section_edge[0]+1,'/',section_edge[1]-section_edge[0]+1)\n",
    "        for xl_num in range(section_edge[2],section_edge[3]+1):\n",
    "            # Make a progres update for the xline number\n",
    "            print('xline-num:',xl_num-section_edge[2]+1,'/',section_edge[3]-section_edge[2]+1)\n",
    "            for z_num in range(section_edge[5]-section_edge[4]+1):\n",
    "                # Call the cube_parse function to get the cubes corresponding to the current point\n",
    "                data[indx*(section_edge[5]-section_edge[4]+1)+z_num,:,:,:,:] = cube_parse(seis_arr = inp_seis,\n",
    "                                                                                        cube_incr = cube_incr,\n",
    "                                                                                        inp_res = inp_res,\n",
    "                                                                                        mode = 'point',\n",
    "                                                                                        padding = False,\n",
    "                                                                                        conc = True,\n",
    "                                                                                        inline_num = il_num,\n",
    "                                                                                        xline_num = xl_num,\n",
    "                                                                                        depth = z_num+section_edge[4])\n",
    "\n",
    "            # Check if we have filled up the data array and need to do a prediction\n",
    "            if (indx+1) % pred_batch == 0:\n",
    "                print('Making prediction on sub-section:')\n",
    "\n",
    "                # Predict the given class or features dependant on the user input\n",
    "                if show_features:\n",
    "                    prediction[jndx*(pred_batch*(section_edge[5]-section_edge[4]+1)):\\\n",
    "                              (jndx+1)*(pred_batch*(section_edge[5]-section_edge[4]+1)),:] = \\\n",
    "                                    intermediate_layer_model.predict((data))\n",
    "\n",
    "                else:\n",
    "                    if show_prob:\n",
    "                        # Simple model prediction with probabilities\n",
    "                        prediction[jndx*(pred_batch*(section_edge[5]-section_edge[4]+1)):\\\n",
    "                                   (jndx+1)*(pred_batch*(section_edge[5]-section_edge[4]+1)),:] = \\\n",
    "                        keras_model.predict((data))\n",
    "                    else:\n",
    "                        # Model prediction of classes\n",
    "                        prediction[jndx*(pred_batch*(section_edge[5]-section_edge[4]+1)):\\\n",
    "                                   (jndx+1)*(pred_batch*(section_edge[5]-section_edge[4]+1)),:] = \\\n",
    "                        np.expand_dims(keras_model.predict_classes((data)),axis = 1)\n",
    "\n",
    "                # Tell the user the section is finished\n",
    "                print('Section finished!')\n",
    "\n",
    "                if jndx == 0:\n",
    "                    # Finish the timer and calculate how long the user should expect the program to take:\n",
    "                    end = time.time()\n",
    "                    DT = end-start # seconds per iteration\n",
    "                    tot_time = num_it*DT+(rem/pred_batch)*DT #seconds\n",
    "\n",
    "                # Give the user an update regarding the time remaining\n",
    "                time_rem = (tot_time-DT*(jndx+1))\n",
    "                if time_rem <= 300:\n",
    "                    print('Approximate time remaining of the prediction:',time_rem, ' sec.')\n",
    "                elif 300 < time_rem <= 60*60:\n",
    "                    minutes = time_rem//60\n",
    "                    seconds = (time_rem%60)*(60/100)\n",
    "                    print('Approximate time remaining of the prediction:',minutes,' min., ',seconds,' sec.')\n",
    "                elif 60*60 < time_rem <= 60*60*24:\n",
    "                    hours = time_rem//(60*60)\n",
    "                    minutes = (time_rem%(60*60))*(1/60)*(60/100)\n",
    "                    print('Approximate time remaining of the prediction:',hours,' hrs., ',minutes,' min., ')\n",
    "                else:\n",
    "                    days = time_rem//(24*60*60)\n",
    "                    hours = (time_rem%(24*60*60))*(1/60)*((1/60))*(24/100)\n",
    "                    print('Approximate time remaining of the prediction:',days,' days, ',hours,' hrs., ')\n",
    "\n",
    "\n",
    "                # Update iterators and give updates to user\n",
    "                indx = 0\n",
    "                jndx+=1\n",
    "                print('Retrieving to memory:')\n",
    "\n",
    "            # Check if we have exhausted the range of data to be predicted and need to finish the function\n",
    "            elif jndx == num_it and indx == rem-1:\n",
    "                # Slice the data array to only include the relevant part\n",
    "                data = data[:indx*(section_edge[5]-section_edge[4]+1)+z_num+1]\n",
    "\n",
    "                print('Finalizing prediction:')\n",
    "\n",
    "                # Make the final prediction\n",
    "                if show_features:\n",
    "                    prediction[jndx*(pred_batch*(section_edge[5]-section_edge[4]+1)):,:] = \\\n",
    "                                    intermediate_layer_model.predict((data))\n",
    "                else:\n",
    "                    if show_prob:\n",
    "                        prediction[jndx*(pred_batch*(section_edge[5]-section_edge[4]+1)):,:] = \\\n",
    "                                    keras_model.predict((data))\n",
    "                    else:\n",
    "                        prediction[jndx*(pred_batch*(section_edge[5]-section_edge[4]+1)):,:] = \\\n",
    "                                    np.expand_dims(keras_model.predict_classes((data)),axis = 1)\n",
    "\n",
    "            # If we should keep filling the data and not predict yet, simply increase the iterator\n",
    "            else:\n",
    "                indx+=1\n",
    "\n",
    "    # Reshape the prediction to the shape of the desired cube\n",
    "    print('Reshaping prediction:')\n",
    "    if show_features:\n",
    "        prediction = prediction.reshape((section_edge[1]-section_edge[0]+1,\\\n",
    "                                         section_edge[3]-section_edge[2]+1,\\\n",
    "                                         section_edge[5]-section_edge[4]+1,10),order='C')\n",
    "    else:\n",
    "        prediction = prediction.reshape((section_edge[1]-section_edge[0]+1,\\\n",
    "                                         section_edge[3]-section_edge[2]+1,\\\n",
    "                                         section_edge[5]-section_edge[4]+1,num_classes),order='C')\n",
    "\n",
    "    print('Prediction finished!', ' shape={}-{}-{}-{}'.format(prediction.shape[0], prediction.shape[1], \n",
    "                                                              prediction.shape[2], prediction.shape[3]))\n",
    "\n",
    "    # Save the prediction as a segy, numpy and csv file\n",
    "    # NOTE: Everything SEGY and CSV is made into 32bit-float to conform to commonly used reading programs\n",
    "    if print_segy:\n",
    "        # Update the data we send to the saver functions dependant on what we have predicted\n",
    "        if show_prob:\n",
    "            class_row = 1\n",
    "        else:\n",
    "            class_row = 0\n",
    "\n",
    "        print('Saving prediction: ...')\n",
    "\n",
    "        # Save the numpy file\n",
    "        np.save(savename + '.npy', prediction)\n",
    "\n",
    "        # Get the right filename in case the input is given as a list\n",
    "        if type(filename) is list:\n",
    "            # Save the segy file using the input filename as a framework\n",
    "            # Just use the first member of the list as the reference\n",
    "            input_file = filename[0]\n",
    "        else:\n",
    "            # Save the segy file using the input filename as a framework\n",
    "            input_file=filename\n",
    "\n",
    "        output_file=savename + '.sgy'\n",
    "\n",
    "        copyfile(input_file, output_file)\n",
    "\n",
    "        with segyio.open( output_file, \"r+\" ) as src:\n",
    "            # iterate through each inline and update the values\n",
    "            i = 0\n",
    "            for ilno in src.ilines:\n",
    "                src.iline[ilno] = -1*(np.ones((src.iline[ilno].shape),dtype = np.float32))\n",
    "\n",
    "                if src.ilines[section_edge[0]] <= ilno <= src.ilines[section_edge[1]]:\n",
    "                    line = src.iline[ilno]\n",
    "                    line[section_edge[2]:section_edge[3]+1,section_edge[4]:section_edge[5]+1] = prediction[i,:,:,class_row]\n",
    "                    src.iline[ilno]=line\n",
    "                    i += 1\n",
    "\n",
    "        # Save the csv(ixz) file\n",
    "        csv_struct(inp_numpy = prediction[:,:,:,class_row],\n",
    "                   spec_obj = seis_obj,\n",
    "                   section = section_edge,\n",
    "                   inp_res = np.float32,\n",
    "                   save = True,\n",
    "                   savename = (savename + '.ixz'))\n",
    "\n",
    "        # Print to the user that the function has finished saving\n",
    "        print('Prediction saved.')\n",
    "\n",
    "    # Return the prediction array\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ---- Functions for visualizing the predictions from the program ----\n",
    "# Make a plotting function for plotting the features\n",
    "def plotNNpred(pred,im_per_line,line_num,section):\n",
    "    # pred: 4D-numpy array with the features in the 4th dimension\n",
    "    # im_per_line: How many sub plot images to have in each row of the display\n",
    "    # line_num: what xline to use as a reference\n",
    "    # section: the section that was used for prediction\n",
    "\n",
    "    # Define some initial parameters, like the number of features and plot size, etc.\n",
    "    features = pred.shape[3]\n",
    "    plt.figure(2, figsize=(20,20))\n",
    "    n_columns = im_per_line\n",
    "    n_rows = math.ceil(features / n_columns) + 1\n",
    "\n",
    "    # Itterate through the sub-plots and fill them with the features, do some simple formatting\n",
    "    for i in range(features):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Feature ' + str(i+1))\n",
    "        plt.imshow(pred[:,line_num-1,:,i].T, interpolation=\"nearest\", cmap=\"rainbow\",\\\n",
    "                   extent=[section[0],section[1],-section[5],-section[4]])\n",
    "        plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make and visualize the predicted data\n",
    "def visualization(filename,inp_seis,seis_obj,keras_model,cube_incr,section_edge,xline_ref,num_classes,\\\n",
    "                  inp_res=np.float64,sect_form = None,save_pred = False, save_file = 'default_write', \\\n",
    "                  pred_batch = 1,show_feature = False, show_prob = True):\n",
    "    # filename: filename of the segy-cube to be imported (necessary for copying the segy-frame before writing a new segy)\n",
    "    # inp_seis: a 3D numpy array that holds the input seismic cube\n",
    "    # seis_obj: Object returned from the segy_decomp function\n",
    "    # keras_model: keras model that has been trained previously\n",
    "    # cube_incr: number of increments included in each direction from the example to make a mini-cube\n",
    "    # section_edge: edge locations of the sub-section; either index or (min. inline, max. inline, min. xline, max xline, min z, max z)\n",
    "    # xline_ref: reference crossline from the original seismic cube to be plotted with the prediction (must be within section)\n",
    "    # num_classes: number of classes to be predicted\n",
    "    # inp_res: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "    # sect_form: formatting of the section edges (if 'segy' we have to convert iline,xline,time to indexes)\n",
    "    # save_pred: whether or not to save the prediction as a segy, numpy and csv(ixz) file.\n",
    "    # save_file: name of the files to be saved (extensions are added automatically)\n",
    "    # pred_batch: number of traces to predict on at a time\n",
    "    # show_features: whether or not to get the features or the classes\n",
    "    # show_prob: if the user wants to get out probabilities or classifications\n",
    "\n",
    "\n",
    "\n",
    "    # Adjust the section numbering and reference xline from inline-xline-time to index if this is given in segy format\n",
    "    if sect_form == 'segy':\n",
    "        section_edge[0] = (section_edge[0] - seis_obj.inl_start)//seis_obj.inl_step\n",
    "        section_edge[1] = (section_edge[1] - seis_obj.inl_start)//seis_obj.inl_step\n",
    "        #\n",
    "        section_edge[2] = (section_edge[2] - seis_obj.xl_start)//seis_obj.xl_step\n",
    "        section_edge[3] = (section_edge[3] - seis_obj.xl_start)//seis_obj.xl_step\n",
    "        #\n",
    "        section_edge[4] = (section_edge[4] - seis_obj.t_start)//seis_obj.t_step\n",
    "        section_edge[5] = (section_edge[5] - seis_obj.t_start)//seis_obj.t_step\n",
    "        #\n",
    "        xline_ref = (xline_ref - seis_obj.xl_start)//seis_obj.xl_step\n",
    "        #\n",
    "        print(' Before adjustment, Section edge geometry after inline/xline conversion: ')\n",
    "        print('ils=', section_edge[0], 'ile=', section_edge[1])\n",
    "        print('xls=', section_edge[2], 'xle=', section_edge[3])\n",
    "        print('zls=', section_edge[4], 'zle=', section_edge[5])\n",
    "        print(' xline_ref={}\\n'.format(xline_ref))\n",
    "\n",
    "    # \n",
    "    print(' inp_seis shape=', inp_seis.shape, ' cube_incr=', cube_incr)\n",
    "    \n",
    "    # need adjusting section\n",
    "    # iline\n",
    "    section_edge[0] = max(section_edge[0], cube_incr)\n",
    "    section_edge[1] = min(section_edge[1], section_edge[1]-cube_incr)\n",
    "    # xline\n",
    "    section_edge[2] = max(section_edge[2], cube_incr)\n",
    "    section_edge[3] = min(section_edge[3], section_edge[3]-cube_incr)\n",
    "    # t\n",
    "    section_edge[4] = max(section_edge[4], cube_incr)\n",
    "    section_edge[5] = min(section_edge[5], section_edge[5]-cube_incr)\n",
    "    #\n",
    "    xline_ref = max(xline_ref, cube_incr)\n",
    "    \n",
    "    #\n",
    "    print(' After adjustment, Section edge geometry after inline/xline conversion: ')\n",
    "    print('ils=', section_edge[0], 'ile=', section_edge[1])\n",
    "    print('xls=', section_edge[2], 'xle=', section_edge[3])\n",
    "    print('zls=', section_edge[4], 'zle=', section_edge[5])\n",
    "    print(' xline_ref={}\\n'.format(xline_ref))\n",
    "        \n",
    "    # Make the prediction\n",
    "    pred = predicting(filename=filename,\n",
    "                      inp_seis=inp_seis,\n",
    "                      seis_obj=seis_obj,\n",
    "                      keras_model=keras_model,\n",
    "                      cube_incr=cube_incr,\n",
    "                      num_classes = num_classes,\n",
    "                      inp_res = inp_res,\n",
    "                      mode='section',\n",
    "                      section=section_edge,\n",
    "                      line_num=0,\n",
    "                      print_segy=save_pred,\n",
    "                      savename=save_file,\n",
    "                      pred_batch = pred_batch,\n",
    "                      show_features=show_feature,\n",
    "                      layer_name='attribute_layer',\n",
    "                      show_prob = show_prob)\n",
    "\n",
    "    # Define some parameters used for getting nice plots(range of c-axis, and which row to show in the prediction)\n",
    "    features = pred.shape[2]\n",
    "    if show_prob:\n",
    "        class_row = 1\n",
    "        c_max = 1\n",
    "    else:\n",
    "        class_row = 0\n",
    "        c_max = num_classes-1\n",
    "\n",
    "    # Visualize the results from the prediction, either with features or classes/probabilities\n",
    "    if show_feature:\n",
    "        # Make the figure object/handle and plot the reference xline\n",
    "        plt.figure(1, figsize=(15,15))\n",
    "        plt.title('x-line')\n",
    "        plt.imshow(inp_seis[cube_incr:-cube_incr,xline_ref,cube_incr:-cube_incr].T,interpolation=\"nearest\",\\\n",
    "                   cmap=\"gray\",extent=[cube_incr,-cube_incr+len(inp_seis),cube_incr-len(inp_seis[0,0]),-cube_incr])\n",
    "\n",
    "        # Plot all the features and show the figures\n",
    "        plotNNpred(pred,5,xline_ref-section_edge[2]+1,section_edge)\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        # Make the figure object/handle and plot the reference xline\n",
    "        plt.figure(1, figsize=(15,15))\n",
    "        gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])\n",
    "        plt.subplot(gs[0])\n",
    "        plt.title('x-line')\n",
    "        plt.imshow(inp_seis[cube_incr:-cube_incr,xline_ref,cube_incr:-cube_incr, 0].T,interpolation=\"nearest\",\\\n",
    "           cmap=\"gray\",extent=[cube_incr,-cube_incr+len(inp_seis),cube_incr-len(inp_seis[0,0]),-cube_incr])\n",
    "        plt.colorbar()\n",
    "\n",
    "        # Plot the probability/classification and show the figures\n",
    "        plt.subplot(gs[1])\n",
    "        plt.title('classification/probability of 1')\n",
    "        plt.imshow(pred[:,xline_ref-section_edge[2],:,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max),\\\n",
    "                  extent=[section_edge[0],section_edge[1],-section_edge[5],-section_edge[4]])\n",
    "\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    # Return the predicted numpy cube\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rough function to show more detailed plots of the predictions in python for QC before going to Petrel\n",
    "def show_details(filename,cube_incr,predic,inline,inl_start,xline,xl_start,\\\n",
    "                 slice_number,slice_incr,inp_format=np.float64,show_prob = True,num_classes = 2):\n",
    "    # filename: filename of the segy-cube to be imported (necessary for copying the segy-frame before writing a new segy)\n",
    "    # cube_incr: number of increments included in each direction from the example to make a mini-cube\n",
    "    # predic: numpy cube holding the prediction\n",
    "    # inline: inline number to center our visualization on\n",
    "    # inl_start: index of the first inline in the prediction\n",
    "    # xline: xline number to center our visualization on\n",
    "    # xl_start: index of the first xline in the prediction\n",
    "    # slice_number: depth slice number to center our visualization on\n",
    "    # slice_incr: increments to take in depth between each plot\n",
    "    # inp_format: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "    # show_prob: if the user wants to get out probabilities or classifications\n",
    "    # num_classes: number of classes that was predicted\n",
    "\n",
    "\n",
    "    # Read out the reference segy object\n",
    "    segy_obj = segy_decomp(segy_file = filename,\n",
    "                           plot_data = False,\n",
    "                           read_direc = 'xline',\n",
    "                           inp_res = inp_format)\n",
    "\n",
    "    # Get the numpy cube from the reference segy object\n",
    "    inp_seis = segy_obj.data\n",
    "\n",
    "    # define some parameters used for getting nice plots(range of c-axis, and which row to show in the prediction)\n",
    "    if show_prob:\n",
    "        class_row = 1\n",
    "        c_max = 1\n",
    "    else:\n",
    "        class_row = 0\n",
    "        c_max = num_classes-1\n",
    "\n",
    "    # Make the figure object/handle and plot the reference xline\n",
    "    plt.figure(1, figsize=(20,15))\n",
    "    plt.subplot(1, 8, 1)\n",
    "    plt.title('xline: ' + str(xline))\n",
    "    plt.imshow(inp_seis[inline-cube_incr:inline+cube_incr,xline,cube_incr:-cube_incr].T,interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Plot the prediciton for the reference xline along with 3 increments in each direction\n",
    "    plt.subplot(1, 8, 2)\n",
    "    plt.title('xline - 3')\n",
    "    plt.imshow(predic[:,xline-xl_start - 3,:,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "    \n",
    "    plt.subplot(1, 8, 3)\n",
    "    plt.title('xline')\n",
    "    plt.imshow(predic[:,xline-xl_start,:,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "\n",
    "    plt.subplot(1, 8, 4)\n",
    "    plt.title('xline + 3')\n",
    "    plt.imshow(predic[:,xline-xl_start + 3,:,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Plot the reference inline\n",
    "    plt.subplot(1, 8, 1+4)\n",
    "    plt.title('inline: ' + str(inline))\n",
    "    plt.imshow(inp_seis[inline,xline-cube_incr:xline+cube_incr,cube_incr:-cube_incr].T,interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Plot the prediciton for the reference inline along with 3 increments in each direction\n",
    "    plt.subplot(1, 8, 2+4)\n",
    "    plt.title('inline - 3')\n",
    "    plt.imshow(predic[inline-inl_start-3,:,:,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "\n",
    "    plt.subplot(1, 8, 3+4)\n",
    "    plt.title('inline')\n",
    "    plt.imshow(predic[inline-inl_start,:,:,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "\n",
    "    plt.subplot(1, 8, 4+4)\n",
    "    plt.title('inline + 3')\n",
    "    plt.imshow(predic[inline-inl_start+3,:,:,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Make a new figure object/handle and plot 3 reference depth slices\n",
    "    plt.figure(2, figsize=(20,5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('slice - ' + str(slice_incr))\n",
    "    plt.imshow(inp_seis[inline-cube_incr:inline+cube_incr,xline-cube_incr:xline+cube_incr,cube_incr+slice_number-slice_incr].T,\\\n",
    "               interpolation=\"nearest\", cmap=\"gray\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('slice: ' + str(slice_number))\n",
    "    plt.imshow(inp_seis[inline-cube_incr:inline+cube_incr,xline-cube_incr:xline+cube_incr,cube_incr+slice_number].T,\\\n",
    "               interpolation=\"nearest\", cmap=\"gray\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('slice + ' + str(slice_incr))\n",
    "    plt.imshow(inp_seis[inline-cube_incr:inline+cube_incr,xline-cube_incr:xline+cube_incr,cube_incr+slice_number+slice_incr].T,\\\n",
    "               interpolation=\"nearest\", cmap=\"gray\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Make a new figure object/handle and plot the 3 corresponding predicted depth slices\n",
    "    plt.figure(3, figsize=(20,5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('slice - ' + str(slice_incr))\n",
    "    plt.imshow(predic[:,:,slice_number-slice_incr,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('slice: ' + str(slice_number))\n",
    "    plt.imshow(predic[:,:,slice_number,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('slice + ' + str(slice_incr))\n",
    "    plt.imshow(predic[:,:,slice_number+slice_incr,class_row].T,interpolation=\"nearest\", cmap=\"gist_rainbow\", clim=(0.0, c_max))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ---- MASTER/MAIN function ----\n",
    "# Make an overall master function that takes inn some basic parameters,\n",
    "# trains, predicts, and visualizes the results from a model\n",
    "def master(segy_filename,inp_format,cube_incr,train_dict={},pred_dict={},mode = 'full'):\n",
    "    # segy_filename: filename of the segy-cube to be imported (necessary for copying the segy-frame before writing a new segy)\n",
    "    # inp_format: input resolution, the formatting of the seismic cube (could be changed to 8-bit data)\n",
    "    # cube_incr: number of increments included in each direction from the example to make a mini-cube\n",
    "    # train_dict: Training parameters packaged as a Python dictionary\n",
    "    # pred_dict: Prediciton parameters packaged as a Python dictionary\n",
    "    # mode: Do we want to train a model('train'), predict using an external model('predict'), or train a model and predict using it('full')\n",
    "\n",
    "    # Implement more than one segy-cube if the input segy_filename is a list\n",
    "    if type(segy_filename) is str or (type(segy_filename) is list and len(segy_filename) == 1):\n",
    "        # Check if the filename needs to be retrieved from a list\n",
    "        if type(segy_filename) is list:\n",
    "            segy_filename = segy_filename[0]\n",
    "\n",
    "        # Make a master segy object\n",
    "        segy_obj = segy_decomp(segy_file = segy_filename,\n",
    "                               plot_data = False,\n",
    "                               read_direc = 'full',\n",
    "                               inp_res = inp_format)\n",
    "\n",
    "        # Define how many segy-cubes we're dealing with\n",
    "        segy_obj.cube_num = 1\n",
    "        segy_obj.data = np.expand_dims(segy_obj.data, axis = 4)\n",
    "\n",
    "    elif type(segy_filename) is list:\n",
    "        # start an iterator\n",
    "        i = 0\n",
    "\n",
    "        # iterate through the list of cube names and store them in a masterobject\n",
    "        for filename in segy_filename:\n",
    "            # Make a master segy object\n",
    "            if i == 0:\n",
    "                segy_obj = segy_decomp(segy_file = filename,\n",
    "                                       plot_data = False,\n",
    "                                       read_direc = 'full',\n",
    "                                       inp_res = inp_format)\n",
    "\n",
    "                # Define how many segy-cubes we're dealing with\n",
    "                segy_obj.cube_num = len(segy_filename)\n",
    "\n",
    "                # Reshape and preallocate the numpy-array for the rest of the cubes\n",
    "                print('Starting restructuring to 4D arrays')\n",
    "                ovr_data = np.empty((list(segy_obj.data.shape) + [len(segy_filename)]))\n",
    "                ovr_data[:,:,:,i] = segy_obj.data\n",
    "                segy_obj.data = ovr_data\n",
    "                ovr_data = None\n",
    "                print('Finished restructuring to 4D arrays')\n",
    "            else:\n",
    "                # Add another cube to the numpy-array\n",
    "                segy_obj.data[:,:,:,i] = segy_adder(segy_file = filename,\n",
    "                                                    inp_cube = segy_obj.data,\n",
    "                                                    read_direc = 'full',\n",
    "                                                    inp_res = inp_format)\n",
    "            # Increase the itterator\n",
    "            i+=1\n",
    "    else:\n",
    "        print('The input filename needs to be a string, or a list of strings')\n",
    "\n",
    "\n",
    "    print('Finished unpaking and restructuring the numpy array')\n",
    "\n",
    "\n",
    "    # Are we going to perform training?\n",
    "    if mode == 'train' or mode == 'full':\n",
    "        # Unpack the dictionary of training parameters\n",
    "        label_list = train_dict['files']\n",
    "        num_bunch = train_dict['num_tot_iterations']\n",
    "        num_epochs = train_dict['epochs']\n",
    "        num_examples = train_dict['num_train_ex']\n",
    "        batch_size = train_dict['batch_size']\n",
    "        opt_patience = train_dict['opt_patience']\n",
    "        data_augmentation = train_dict['data_augmentation']\n",
    "        write_out = train_dict['save_model']\n",
    "        write_location = train_dict['save_location']\n",
    "\n",
    "        # If there is a model given in the prediction dictionary continue training on this model\n",
    "        if 'keras_model' in pred_dict:\n",
    "            keras_model = pred_dict['keras_model']\n",
    "        else:\n",
    "            keras_model = None\n",
    "\n",
    "        # Print out an initial statement to confirm the parameters(QC)\n",
    "        print('num full iterations:', num_bunch)\n",
    "        print('num epochs:',num_epochs)\n",
    "        print('num examples per epoch:',num_examples)\n",
    "        print('batch size:',batch_size)\n",
    "        print('optimizer patience:',opt_patience)\n",
    "\n",
    "\n",
    "        # Make the list of class data\n",
    "        print('Making class-adresses')\n",
    "        class_array = convert(file_list = label_list,\n",
    "                              save = False,\n",
    "                              savename = None,\n",
    "                              ex_adjust = True)\n",
    "\n",
    "        print('Finished making class-adresses')\n",
    "\n",
    "        # Time the training process\n",
    "        start_train_time = time.time()\n",
    "\n",
    "        # Train a new model/further train the uploaded model and store the result as the model output\n",
    "        model = train_model(segy_obj = segy_obj,\n",
    "                            class_array = class_array,\n",
    "                            num_classes = len(label_list),\n",
    "                            cube_incr = cube_incr,\n",
    "                            inp_res = inp_format,\n",
    "                            num_bunch = num_bunch,\n",
    "                            num_epochs = num_epochs,\n",
    "                            num_examples = num_examples,\n",
    "                            batch_size = batch_size,\n",
    "                            opt_patience = opt_patience,\n",
    "                            data_augmentation = data_augmentation,\n",
    "                            num_channels = segy_obj.cube_num,\n",
    "                            keras_model = keras_model,\n",
    "                            write_out = write_out,\n",
    "                            write_location = write_location)\n",
    "\n",
    "        # Time the training process\n",
    "        end_train_time = time.time()\n",
    "        train_time = end_train_time-start_train_time # seconds\n",
    "\n",
    "        # print to the user the total time spent training\n",
    "        if train_time <= 300:\n",
    "            print('Total time elapsed during training:',train_time, ' sec.')\n",
    "        elif 300 < train_time <= 60*60:\n",
    "            minutes = train_time//60\n",
    "            seconds = (train_time%60)*(60/100)\n",
    "            print('Total time elapsed during training:',minutes,' min., ',seconds,' sec.')\n",
    "        elif 60*60 < train_time <= 60*60*24:\n",
    "            hours = train_time//(60*60)\n",
    "            minutes = (train_time%(60*60))*(1/60)*(60/100)\n",
    "            print('Total time elapsed during training:',hours,' hrs., ',minutes,' min., ')\n",
    "        else:\n",
    "            days = train_time//(24*60*60)\n",
    "            hours = (train_time%(24*60*60))*(1/60)*((1/60))*(24/100)\n",
    "            print('Total time elapsed during training:',days,' days, ',hours,' hrs., ')\n",
    "\n",
    "    elif mode == 'predict':\n",
    "        # If we aren't performing any training\n",
    "        print('Using uploaded model for prediction')\n",
    "    else:\n",
    "        print('Invalid mode! Accepted inputs are ''train'', ''predict'', or ''full''')\n",
    "        return None\n",
    "\n",
    "    # Are we going to perform prediction?\n",
    "    if mode == 'predict' or mode == 'full':\n",
    "        # Let the user know if we have made new computations on the model used for prediction\n",
    "        if mode == 'full':\n",
    "            print('Using the newly computed model for prediction')\n",
    "        else:\n",
    "            model  = pred_dict['keras_model']\n",
    "\n",
    "        # Unpack the prediction dictionary\n",
    "        section_edge  = pred_dict['section_edge']\n",
    "        xline_ref = pred_dict['xline']\n",
    "        num_classes = pred_dict['num_class']\n",
    "        sect_form = pred_dict['cord_syst']\n",
    "        show_feature  = pred_dict['show_feature']\n",
    "        save_pred = pred_dict['save_pred']\n",
    "        save_loc = pred_dict['save_location']\n",
    "        pred_batch = pred_dict['pred_batch']\n",
    "        prob = pred_dict['pred_prob']\n",
    "\n",
    "        # Time the prediction process\n",
    "        start_pred_time = time.time()\n",
    "\n",
    "        # Make a prediction on the master segy object using the desired model, and plot the results\n",
    "        pred = visualization(filename = segy_filename,\n",
    "                             inp_seis = segy_obj.data,\n",
    "                             seis_obj = segy_obj,\n",
    "                             keras_model = model,\n",
    "                             cube_incr = cube_incr,\n",
    "                             section_edge = section_edge,\n",
    "                             xline_ref = xline_ref,\n",
    "                             num_classes = num_classes,\n",
    "                             inp_res = inp_format,\n",
    "                             sect_form = sect_form,\n",
    "                             save_pred = save_pred,\n",
    "                             save_file = save_loc,\n",
    "                             pred_batch = pred_batch,\n",
    "                             show_feature = show_feature,\n",
    "                             show_prob = prob)\n",
    "\n",
    "        # Print the time taken for the prediction\n",
    "        end_pred_time = time.time()\n",
    "        pred_time = end_pred_time-start_pred_time # seconds\n",
    "\n",
    "        # print to the user the total time spent training\n",
    "        if pred_time <= 300:\n",
    "            print('Total time elapsed during prediction:',pred_time, ' sec.')\n",
    "        elif 300 < pred_time <= 60*60:\n",
    "            minutes = pred_time//60\n",
    "            seconds = (pred_time%60)*(60/100)\n",
    "            print('Total time elapsed during prediction:',minutes,' min., ',seconds,' sec.')\n",
    "        elif 60*60 < pred_time <= 60*60*24:\n",
    "            hours = pred_time//(60*60)\n",
    "            minutes = (pred_time%(60*60))*(1/60)*(60/100)\n",
    "            print('Total time elapsed during prediction:',hours,' hrs., ',minutes,' min., ')\n",
    "        else:\n",
    "            days = pred_time//(24*60*60)\n",
    "            hours = (pred_time%(24*60*60))*(1/60)*((1/60))*(24/100)\n",
    "            print('Total time elapsed during prediction:',days,' days, ',hours,' hrs., ')\n",
    "\n",
    "    else:\n",
    "        # Make an empty variable for the prediction output\n",
    "        pred = None\n",
    "\n",
    "    # Return the new model and/or prediction as an output dictionary\n",
    "    output = {\n",
    "        'model' : model,\n",
    "        'pred' : pred\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### ---- Run an instance of the master function ----\n",
    "\n",
    "# Run the master function and save the output in the output dictionary output_dict\n",
    "output_dict1 = master(\n",
    "    segy_filename = filenames,     # Seismic filenames\n",
    "    inp_format = inp_res,     # Format of input seismic\n",
    "    cube_incr = cube_incr,     # Increments in each direction to create a training cube\n",
    "    train_dict = train_dict,     # Input training dictionary\n",
    "    pred_dict = pred_dict,     # Input prediction dictionary\n",
    "    mode = 'predict'     # Input mode ('train', 'predict', or 'full' for both training AND prediction)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = output_dict1['pred']\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show additional details about the prediciton\n",
    "show_details(\n",
    "    filenames[0],\n",
    "    cube_incr,\n",
    "    pred,\n",
    "    inline = 400,\n",
    "    inl_start = 356,\n",
    "    xline = 400,\n",
    "    xl_start = 400,\n",
    "    slice_number = 200,\n",
    "    slice_incr = 3,\n",
    "    show_prob=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Save/load functions\n",
    "## returns a prediction cube\n",
    "## identical to the one saved\n",
    "#prediction = np.load('filename.npy')\n",
    "#\n",
    "## returns a compiled model\n",
    "## identical to the one saved\n",
    "#loaded_model = keras.models.load_model('filename.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
